import os

import random
import json

from transformers import AutoTokenizer

from config import DATA_CACHE_PATH, INSTRUCTION_DATA_PATH, MODEL_NAME, MODEL_CACHE_PATH, PROMPT_TEMPLATE

from datasets import load_dataset

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=MODEL_CACHE_PATH)

def add_dataset(
    all_data,
    name,
    path,
    tokenizer,
    question_key_en,
    answer_key,
    *,
    mask_key=None,
    question_key_vi=None,
    subset=None,
    split="train",
    n_samples=0,
    max_length=512,
):
    """
    T·∫£i 1 dataset t·ª´ Hugging Face, l·ªçc c√¢u qu√° d√†i theo tokenizer,
    v√† th√™m v√†o list d·ªØ li·ªáu chung.
    """

    print(f"üì• Loading {name}...")
    ds = load_dataset(path, subset, split=split, cache_dir=DATA_CACHE_PATH)
    ds = ds.shuffle(seed=42)
    if n_samples == 0:
        n_samples = len(ds)
    # L·∫•y nhi·ªÅu h∆°n n_samples ƒë·ªÉ l·ªçc b·ªõt nh·ªØng c√¢u d√†i

    count = 0
    for ex in ds:
        if count >= n_samples:
            break

        if question_key_vi is not None:
            if count % 2 == 0:
                q = ex.get(question_key_en)
            else:
                q = ex.get(question_key_vi)
        else:
            q = ex.get(question_key_en)

        a = ex.get(answer_key)
        if not q or not a:
            continue

        # lines = a.strip().splitlines()
        # last = lines[-1]
        # if last.startswith("The answer is:"):
        #     answer = last.replace("The answer is:", "").strip()
        #     lines[-1] = f"The answer is: \\boxed{{{answer}}}"
        # a = "\n".join(lines)

        # T·∫°o text k·∫øt h·ª£p instruction + output ƒë·ªÉ check ƒë·ªô d√†i token
        text = PROMPT_TEMPLATE.format(question='q') + a
        tokens = tokenizer(text, truncation=False)
        if len(tokens["input_ids"]) > max_length:
            continue

        data = {
            "instruction": str(q).strip(),
            "response": str(a).strip()
        }
        if mask_key is not None:
            data["mask"] = str(ex.get(mask_key)).strip()

        all_data.append(data)
        count += 1

def build_small_math_reasoning(output_dir=".", test_ratio=0.1):
    """T·∫°o dataset reasoning to√°n h·ªçc v√† chia train/val/test."""

    data = []

    # Dataset ch√≠nh
    add_dataset(
        data,
        name="mathqa",
        tokenizer=tokenizer,
        path="nlile/hendrycks-MATH-benchmark",
        question_key_en="problem",
        answer_key="solution",
        n_samples=2000,
        max_length=512
    )
    add_dataset(
        data,
        name="numinamath",
        tokenizer=tokenizer,
        path="AI-MO/NuminaMath-CoT",
        question_key_en="problem",
        answer_key="solution",
        n_samples=1500,
        max_length=1024
    )
    # add_dataset(
    #     data,
    #     name="open_math_masked",
    #     tokenizer=tokenizer,
    #     path="nvidia/OpenMath-MATH-masked",
    #     question_key_en="question",
    #     answer_key="reference_solution",
    #     mask_key="masked_reference_solution",
    #     max_length=1024
    # )

    random.shuffle(data)
    total = len(data)

    # üßÆ Chia t·∫≠p (80% train, 10% val, 10% test)
    train_end = int((1-test_ratio) * total)

    splits = {
        "train": data[:train_end],
        "test": data[train_end:]
    }

    os.makedirs(output_dir, exist_ok=True)

    for split_name, split_data in splits.items():
        path = os.path.join(output_dir, f"{split_name}.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(split_data, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ Saved {len(split_data)} samples to {path}")

    print(f"\nüèÅ Done! Total: {total} samples")


if __name__ == "__main__":
    build_small_math_reasoning(output_dir=INSTRUCTION_DATA_PATH, test_ratio=0)