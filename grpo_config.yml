model:
  name: "Qwen/Qwen3-1.7B"

training:
  output_dir: "./grpo-cot-model"
  learning_rate: 1e-6
  min_lr_rate: 0.1
  batch_size: 4
  epochs: 2
  weight_decay: 0.01
  gradient_accumulation_steps: 4
  logging_steps: 30
  eval_steps: 200
  save_steps: 200
  save_total_limit: 3
  warmup_ratio: 0.05

vllm:
  enabled: True
  gpu_memory_utilization: 0.3
  max_model_len: 4096

grpo:
  num_generations: 4
  max_completion_length: 3096
  max_backtracking_word: 20
  temperature: 0.7
  top_p: 0.9

  reward_funcs:
    # "accuracy": 2.0
    format: 1.0
    # "reasoning_steps": 0.5
    # length: 0.3
    # cosine: 2.0
    cosine_word: 2.0
    repetition_penalty: 0.1

  # Tham số reward nâng cao
  cosine_min_value_wrong: 0.0
  cosine_max_value_wrong: -0.5
  cosine_min_value_correct: 0.5
  cosine_max_value_correct: 1.0
  cosine_max_len: 12000 
  cosine_max_word: 15

  repetition_n_grams: 3
  repetition_max_penalty: -0.5

lora:
  using_lora: True
  sft_model_path: "sft-cot-model/best_model"
  r: 32
  lora_alpha: 64
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  lora_dropout: 0.05
  bias: "none"

dataset:
  train_path: "./data/val.json"
  val_ratio: 0.05
