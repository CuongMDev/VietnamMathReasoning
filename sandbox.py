import torch
import tempfile
import subprocess
import threading
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer


# ======================================================
# 1Ô∏è‚É£ Load model
# ======================================================
def load_model(model_name: str, device="auto"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map=device
    )
    return model, tokenizer


# ======================================================
# 2Ô∏è‚É£ Sinh code Python t·ª´ prompt (h·ªó tr·ª£ batch + streamer)
# ======================================================
def generate_code_batch(model, tokenizer, prompts, max_new_tokens=256):
    streamer = TextIteratorStreamer(
        tokenizer, skip_prompt=True, skip_special_tokens=True
    )
    inputs = tokenizer(prompts, return_tensors="pt", padding=True).to(model.device)

    generation_kwargs = dict(
        **inputs,
        streamer=streamer,
        max_new_tokens=max_new_tokens,
        do_sample=False
    )

    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    full_text = ""
    for chunk in streamer:
        print(chunk, end="", flush=True)
        full_text += chunk

    thread.join()
    return full_text


# ======================================================
# 3Ô∏è‚É£ H√†m th·ª±c thi code trong sandbox
# ======================================================
def run_in_sandbox(code: str, timeout: float = 3.0) -> str:
    """Ch·∫°y code Python trong m√¥i tr∆∞·ªùng c√¥ l·∫≠p, l·∫•y stdout ho·∫∑c stderr"""
    with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
        f.write(code)
        tmp_path = f.name

    try:
        result = subprocess.run(
            ["python", tmp_path],
            capture_output=True,
            text=True,
            timeout=timeout
        )
        output = result.stdout.strip() or result.stderr.strip()
    except subprocess.TimeoutExpired:
        output = "‚ö†Ô∏è Timeout: code ch·∫°y qu√° l√¢u"
    except Exception as e:
        output = f"‚ö†Ô∏è L·ªói sandbox: {e}"

    return output


# ======================================================
# 4Ô∏è‚É£ Pipeline ho√†n ch·ªânh: t·ª´ prompt ‚Üí code ‚Üí k·∫øt qu·∫£ th·ª±c thi
# ======================================================
def run_pipeline(model, tokenizer, prompts):
    codes = []
    outputs = []
    print("\n=== üîπ Sinh code t·ª´ m√¥ h√¨nh ===")
    generated = generate_code_batch(model, tokenizer, prompts)

    # N·∫øu model sinh nhi·ªÅu code, b·∫°n c√≥ th·ªÉ c·∫Øt b·∫±ng <|end|> ho·∫∑c d·∫•u ```output
    codes = [generated.strip()]

    print("\n\n=== üîπ Th·ª±c thi sandbox ===")
    for i, code in enumerate(codes):
        print(f"\nüßÆ Batch {i}:")
        result = run_in_sandbox(code)
        print(result)
        outputs.append(result)

    return outputs


# ======================================================
# 5Ô∏è‚É£ V√≠ d·ª• s·ª≠ d·ª•ng
# ======================================================
if __name__ == "__main__":
    model_name = "Qwen/Qwen2-1.5B-Instruct"
    model, tokenizer = load_model(model_name)

    prompts = [
        """Vi·∫øt code Python sau:
T√≠nh th·ªÉ t√≠ch t·ª© di·ªán c√≥ ma tr·∫≠n Cayley-Menger nh∆∞ sau:
C = np.array([
    [0, 1, 1, 1, 1],
    [1, 0, 41, 80, 89],
    [1, 41, 0, 80, 89],
    [1, 80, 80, 0, 89],
    [1, 89, 89, 89, 0]
])
Sau ƒë√≥ in ra th·ªÉ t√≠ch V.
""",
    ]

    run_pipeline(model, tokenizer, prompts)
